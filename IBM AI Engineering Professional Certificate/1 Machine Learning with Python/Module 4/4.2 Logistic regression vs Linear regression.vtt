WEBVTT

1
00:00:00.040 --> 00:00:03.960
Hello, and welcome.
In this video,

2
00:00:03.960 --> 00:00:05.600
we will learn the
difference between

3
00:00:05.600 --> 00:00:08.540
linear regression and
logistic regression.

4
00:00:08.540 --> 00:00:11.920
We go over linear regression
and see why it cannot

5
00:00:11.920 --> 00:00:15.600
be used properly for some
binary classification problems.

6
00:00:15.600 --> 00:00:18.060
We also look at the
sigmoid function,

7
00:00:18.060 --> 00:00:20.880
which is the main part
of logistic regression.

8
00:00:20.880 --> 00:00:23.630
Let's start. Let's look

9
00:00:23.630 --> 00:00:26.510
at the telecommunication
dataset again.

10
00:00:26.510 --> 00:00:28.930
The goal of logistic
regression is

11
00:00:28.930 --> 00:00:30.970
to build a model to
predict the class of

12
00:00:30.970 --> 00:00:33.610
each customer and
also the probability

13
00:00:33.610 --> 00:00:36.220
of each sample
belonging to a class.

14
00:00:36.220 --> 00:00:38.730
Ideally, we want
to build a model

15
00:00:38.730 --> 00:00:40.230
y hat that can

16
00:00:40.230 --> 00:00:43.010
estimate that the class
of a customer is one,

17
00:00:43.010 --> 00:00:45.970
given its features x. I

18
00:00:45.970 --> 00:00:49.410
want to emphasize that
y is the labels vector,

19
00:00:49.410 --> 00:00:53.520
also called actual values that
we would like to predict,

20
00:00:53.520 --> 00:00:55.550
and y hat is the vector

21
00:00:55.550 --> 00:00:58.400
of the predicted
values by our model.

22
00:00:58.400 --> 00:01:01.570
Mapping the class labels
to integer numbers,

23
00:01:01.570 --> 00:01:05.165
can we use linear regression
to solve this problem?

24
00:01:05.165 --> 00:01:07.370
First, let's recall how

25
00:01:07.370 --> 00:01:08.910
linear regression works to

26
00:01:08.910 --> 00:01:11.430
better understand
logistic regression.

27
00:01:11.430 --> 00:01:14.530
Forget about the churn prediction
for a minute and assume

28
00:01:14.530 --> 00:01:15.670
our goal is to predict

29
00:01:15.670 --> 00:01:18.415
the income of customers
in the dataset.

30
00:01:18.415 --> 00:01:21.040
This means that instead
of predicting churn,

31
00:01:21.040 --> 00:01:23.000
which is a categorical value,

32
00:01:23.000 --> 00:01:25.100
let's predict income, which is

33
00:01:25.100 --> 00:01:28.780
a continuous value.
How can we do this?

34
00:01:28.780 --> 00:01:31.210
Let's select an
independent variable,

35
00:01:31.210 --> 00:01:33.040
such as customer age and

36
00:01:33.040 --> 00:01:36.160
predict a dependent
variable, such as income.

37
00:01:36.160 --> 00:01:38.540
Of course, we can
have more features,

38
00:01:38.540 --> 00:01:40.220
but for the sake of simplicity,

39
00:01:40.220 --> 00:01:42.280
let's just take
one feature here.

40
00:01:42.280 --> 00:01:45.890
We can plot it and show age
as an independent variable,

41
00:01:45.890 --> 00:01:49.295
and income as the target value
we would like to predict.

42
00:01:49.295 --> 00:01:51.770
With linear regression,
you can fit

43
00:01:51.770 --> 00:01:54.590
a line or polynomial
through the data.

44
00:01:54.590 --> 00:01:56.870
We can find this line
through training

45
00:01:56.870 --> 00:02:00.150
our model or calculating
it mathematically,

46
00:02:00.150 --> 00:02:02.070
based on the sample sets.

47
00:02:02.070 --> 00:02:05.705
We'll say this is a straight
line through the sample set.

48
00:02:05.705 --> 00:02:10.180
This line has an equation
shown as a+bx_1.

49
00:02:10.470 --> 00:02:15.490
Now, use this line to predict
the continuous value y.

50
00:02:15.490 --> 00:02:18.870
That is, use this line
to predict the income of

51
00:02:18.870 --> 00:02:21.090
an unknown customer based on

52
00:02:21.090 --> 00:02:24.405
his or her age, and it is done.

53
00:02:24.405 --> 00:02:27.345
What if we want
to predict churn?

54
00:02:27.345 --> 00:02:29.800
Can we use the same
technique to predict

55
00:02:29.800 --> 00:02:33.615
a categorical field such
as churn? Let's see.

56
00:02:33.615 --> 00:02:36.340
Say we're given data
on customer churn,

57
00:02:36.340 --> 00:02:38.320
and our goal this
time is to predict

58
00:02:38.320 --> 00:02:40.960
the churn of customers
based on their age.

59
00:02:40.960 --> 00:02:44.780
We have a feature,
age denoted as x_1,

60
00:02:44.780 --> 00:02:46.560
and a categorical feature,

61
00:02:46.560 --> 00:02:48.905
churn with two classes,

62
00:02:48.905 --> 00:02:51.705
churn is yes, and churn is no.

63
00:02:51.705 --> 00:02:54.220
As mentioned, we can map yes and

64
00:02:54.220 --> 00:02:57.255
no to integer values
zero and one.

65
00:02:57.255 --> 00:02:59.700
How can we model it now?

66
00:02:59.700 --> 00:03:01.910
Well, graphically, we

67
00:03:01.910 --> 00:03:04.410
could represent our data
with a scatter plot,

68
00:03:04.410 --> 00:03:08.565
but this time, we have only
two values for the y-axis.

69
00:03:08.565 --> 00:03:12.270
In this plot, Class
0 is denoted in red,

70
00:03:12.270 --> 00:03:14.970
and Class 1 is denoted in blue.

71
00:03:14.970 --> 00:03:18.130
Our goal here is to
make a model based on

72
00:03:18.130 --> 00:03:20.170
existing data to predict if

73
00:03:20.170 --> 00:03:22.830
a new customer is red or blue.

74
00:03:22.830 --> 00:03:25.110
Let's do the same
technique that we

75
00:03:25.110 --> 00:03:27.450
used for linear
regression here to

76
00:03:27.450 --> 00:03:29.170
see if we can solve
the problem for

77
00:03:29.170 --> 00:03:32.440
a categorical attribute
such as churn.

78
00:03:32.440 --> 00:03:35.550
With linear regression,
you, again,

79
00:03:35.550 --> 00:03:38.410
can fit a polynomial
through the data,

80
00:03:38.410 --> 00:03:42.150
which is shown
traditionally as a+bx.

81
00:03:42.830 --> 00:03:45.870
This polynomial can also be

82
00:03:45.870 --> 00:03:50.615
shown traditionally as
Theta_0+Theta_1x_1.

83
00:03:50.615 --> 00:03:53.430
This line has two parameters

84
00:03:53.430 --> 00:03:55.630
which are shown
with vector Theta,

85
00:03:55.630 --> 00:04:00.115
where the values of the vector
are Theta_0 and Theta_1.

86
00:04:00.115 --> 00:04:02.810
We can also show the
equation of this line

87
00:04:02.810 --> 00:04:06.400
formally as Theta transpose X.

88
00:04:06.400 --> 00:04:08.370
Generally, we can show

89
00:04:08.370 --> 00:04:11.105
the equation for a
multidimensional space,

90
00:04:11.105 --> 00:04:13.530
as Theta transpose X,

91
00:04:13.530 --> 00:04:16.380
where Theta is the
parameters of the line in

92
00:04:16.380 --> 00:04:19.250
two-dimensional space
or parameters of

93
00:04:19.250 --> 00:04:22.870
a plane in three-dimensional
space, and so on.

94
00:04:22.870 --> 00:04:24.710
As Theta is a vector of

95
00:04:24.710 --> 00:04:28.230
parameters and is supposed
to be multiplied by x,

96
00:04:28.230 --> 00:04:32.590
it is shown conventionally
as transpose Theta.

97
00:04:32.590 --> 00:04:36.250
Theta is also called
the weights factor or

98
00:04:36.250 --> 00:04:38.630
confidences of the equation

99
00:04:38.630 --> 00:04:41.970
with both these terms
used interchangeably,

100
00:04:41.970 --> 00:04:44.210
and x is the feature set,

101
00:04:44.210 --> 00:04:46.315
which represents a customer.

102
00:04:46.315 --> 00:04:49.000
Anyway, given a dataset,

103
00:04:49.000 --> 00:04:51.195
all the feature sets, x,

104
00:04:51.195 --> 00:04:54.200
Theta parameters can
be calculated through

105
00:04:54.200 --> 00:04:57.620
an optimization algorithm
or mathematically,

106
00:04:57.620 --> 00:05:00.580
which results in the equation
of the fitting line.

107
00:05:00.580 --> 00:05:07.100
For example, the parameters
of this line are -1 and 0.1,

108
00:05:07.100 --> 00:05:11.370
and the equation for
the line is -1+0.1x_1.

109
00:05:14.090 --> 00:05:16.290
Now, we can use

110
00:05:16.290 --> 00:05:17.790
this regression line to predict

111
00:05:17.790 --> 00:05:19.665
the churn of the new customer.

112
00:05:19.665 --> 00:05:22.590
For example, for our customer,

113
00:05:22.590 --> 00:05:27.240
or let's say a data point with
x value of age equals 13,

114
00:05:27.240 --> 00:05:30.300
we can plug the value
into the line formula

115
00:05:30.300 --> 00:05:34.125
and the y value is calculated
and returns a number.

116
00:05:34.125 --> 00:05:36.795
For instance, for p_1 point,

117
00:05:36.795 --> 00:05:41.130
we have Theta transpose
x equals -1+0.1*x_1

118
00:05:41.130 --> 00:05:49.800
= -1+0.1*13

119
00:05:49.800 --> 00:05:52.020
= 0.3.

120
00:05:52.020 --> 00:05:54.420
We can show it on our graph.

121
00:05:54.420 --> 00:05:57.195
Now we can define
a threshold here.

122
00:05:57.195 --> 00:06:01.965
For example, at 0.5
to define the class.

123
00:06:01.965 --> 00:06:05.805
We write a rule here
for our model, y hat,

124
00:06:05.805 --> 00:06:09.615
which allows us to separate
class 0 from class 1.

125
00:06:09.615 --> 00:06:14.160
If the value of Theta
transpose x is less than 0.5,

126
00:06:14.160 --> 00:06:15.975
then the class is zero.

127
00:06:15.975 --> 00:06:17.910
Otherwise, if the value of

128
00:06:17.910 --> 00:06:21.165
Theta transpose x
is more than 0.5,

129
00:06:21.165 --> 00:06:23.235
then the class is one.

130
00:06:23.235 --> 00:06:25.995
Because our customer's y value

131
00:06:25.995 --> 00:06:27.555
is less than the threshold,

132
00:06:27.555 --> 00:06:31.455
we can say it belongs to
class 0 based on our model.

133
00:06:31.455 --> 00:06:33.705
But there is one problem here.

134
00:06:33.705 --> 00:06:35.580
What is the probability that

135
00:06:35.580 --> 00:06:38.010
this customer
belongs to class 0?

136
00:06:38.010 --> 00:06:39.840
As you can see, it's not

137
00:06:39.840 --> 00:06:42.270
the best model to
solve this problem.

138
00:06:42.270 --> 00:06:46.050
Also, there are some
other issues which verify

139
00:06:46.050 --> 00:06:47.850
that linear regression is not

140
00:06:47.850 --> 00:06:51.540
the proper method for
classification problems.

141
00:06:51.540 --> 00:06:53.730
As mentioned, if we use

142
00:06:53.730 --> 00:06:56.685
the regression line to
calculate the class of a point,

143
00:06:56.685 --> 00:07:02.580
it always returns a number
such as 3 or -2, and so on.

144
00:07:02.580 --> 00:07:04.935
Then we should use a threshold.

145
00:07:04.935 --> 00:07:07.620
For example, 0.5 to assign

146
00:07:07.620 --> 00:07:11.115
that point to either
class of zero or one.

147
00:07:11.115 --> 00:07:14.805
This threshold works as
a step function that

148
00:07:14.805 --> 00:07:19.410
outputs zero or one regardless
of how big or small,

149
00:07:19.410 --> 00:07:22.755
positive or negative
the input is.

150
00:07:22.755 --> 00:07:27.465
Using the threshold, we can
find the class of a record.

151
00:07:27.465 --> 00:07:29.625
Notice that in the
step function,

152
00:07:29.625 --> 00:07:31.215
no matter how big the value

153
00:07:31.215 --> 00:07:34.485
is as long as it's
greater than 0.5,

154
00:07:34.485 --> 00:07:38.055
it simply equals
one and vice versa,

155
00:07:38.055 --> 00:07:41.460
regardless of how
small the value y is,

156
00:07:41.460 --> 00:07:45.625
the output would be zero
if it is less than 0.5.

157
00:07:45.625 --> 00:07:48.620
In other words, there is
no difference between

158
00:07:48.620 --> 00:07:52.115
a customer who has a
value of one or 1,000.

159
00:07:52.115 --> 00:07:54.355
The outcome would be one.

160
00:07:54.355 --> 00:07:57.150
Instead of having
this step function,

161
00:07:57.150 --> 00:08:00.045
wouldn't it be nice if
we had a smother line.

162
00:08:00.045 --> 00:08:03.975
One that would project these
values between 0 and 1.

163
00:08:03.975 --> 00:08:07.260
Indeed, the existing method
does not really give

164
00:08:07.260 --> 00:08:10.365
us the probability of a
customer belonging to a class,

165
00:08:10.365 --> 00:08:12.315
which is very desirable.

166
00:08:12.315 --> 00:08:14.340
We need a method
that can give us

167
00:08:14.340 --> 00:08:17.490
the probability of falling
in a class as well.

168
00:08:17.490 --> 00:08:20.865
What is the scientific
solution here?

169
00:08:20.865 --> 00:08:24.975
Well, if instead of
using Theta transpose x,

170
00:08:24.975 --> 00:08:28.065
we use a specific
function called sigmoid.

171
00:08:28.065 --> 00:08:30.735
Then sigmoid of Theta transpose

172
00:08:30.735 --> 00:08:32.880
x gives us the probability of

173
00:08:32.880 --> 00:08:34.920
a point belonging to a class

174
00:08:34.920 --> 00:08:38.010
instead of the value
of y directly.

175
00:08:38.010 --> 00:08:41.190
I'll explain this sigmoid
function in a second.

176
00:08:41.190 --> 00:08:44.790
But for now, please accept
that it will do the trick.

177
00:08:44.790 --> 00:08:47.190
Instead of calculating
the value of

178
00:08:47.190 --> 00:08:49.500
Theta transpose x directly,

179
00:08:49.500 --> 00:08:52.020
it returns the
probability that a

180
00:08:52.020 --> 00:08:56.130
Theta transpose x is
very big or very small.

181
00:08:56.130 --> 00:08:59.820
It always returns a
value between 0 and 1

182
00:08:59.820 --> 00:09:04.695
depending on how large the
Theta transpose x actually is.

183
00:09:04.695 --> 00:09:08.985
Now, our model is sigmoid
of Theta transpose x,

184
00:09:08.985 --> 00:09:10.890
which represents the
probability that

185
00:09:10.890 --> 00:09:13.530
the output is one given x.

186
00:09:13.530 --> 00:09:15.675
Now, the question is,

187
00:09:15.675 --> 00:09:18.255
what is the sigmoid function?

188
00:09:18.255 --> 00:09:22.440
Let me explain in detail
what sigmoid really is.

189
00:09:22.440 --> 00:09:25.050
The sigmoid function,
also called

190
00:09:25.050 --> 00:09:27.330
the logistic function resembles

191
00:09:27.330 --> 00:09:29.190
the step function and is used by

192
00:09:29.190 --> 00:09:32.715
the following expression in
the logistic regression.

193
00:09:32.715 --> 00:09:36.690
The sigmoid function looks
a bit complicated at first,

194
00:09:36.690 --> 00:09:39.450
but don't worry about
remembering this equation.

195
00:09:39.450 --> 00:09:42.130
It'll make sense to you
after working with it.

196
00:09:42.130 --> 00:09:44.915
Notice that in the
sigmoid equation,

197
00:09:44.915 --> 00:09:48.260
when Theta transpose
x gets very big,

198
00:09:48.260 --> 00:09:50.120
the e power minus theta

199
00:09:50.120 --> 00:09:53.315
transpose x in the
denominator of the fraction

200
00:09:53.315 --> 00:09:55.430
becomes almost zero and

201
00:09:55.430 --> 00:09:58.915
the value of the sigmoid
function gets closer to one.

202
00:09:58.915 --> 00:10:02.460
If Theta transpose
x is very small,

203
00:10:02.460 --> 00:10:05.745
the sigmoid function
gets closer to zero.

204
00:10:05.745 --> 00:10:08.835
Depicting on the sigmoid plot,

205
00:10:08.835 --> 00:10:11.580
when Theta transpose
x gets bigger,

206
00:10:11.580 --> 00:10:15.300
the value of the sigmoid
function gets closer to one.

207
00:10:15.300 --> 00:10:19.380
Also, if the Theta
transpose x is very small,

208
00:10:19.380 --> 00:10:22.650
the sigmoid function
gets closer to zero.

209
00:10:22.650 --> 00:10:27.765
The sigmoid functions output
is always between 0 and 1,

210
00:10:27.765 --> 00:10:29.240
which makes it proper to

211
00:10:29.240 --> 00:10:31.920
interpret the results
as probabilities.

212
00:10:31.920 --> 00:10:34.450
It is obvious that
when the outcome of

213
00:10:34.450 --> 00:10:36.835
the sigmoid function
gets closer to 1,

214
00:10:36.835 --> 00:10:41.620
the probability(y)=1
given x goes up.

215
00:10:41.620 --> 00:10:45.865
In contrast, when the sigmoid
value is closer to zero,

216
00:10:45.865 --> 00:10:50.920
the probability(y)=1,
given x, is very small.

217
00:10:50.920 --> 00:10:52.930
What is the output of

218
00:10:52.930 --> 00:10:55.660
our model when we use
the sigmoid function?

219
00:10:55.660 --> 00:10:59.380
In logistic regression, we
model the probability that

220
00:10:59.380 --> 00:11:03.700
an input x belongs to
the default class,

221
00:11:03.700 --> 00:11:06.280
y=1, and we can
write this formally

222
00:11:06.280 --> 00:11:10.165
as probability(y)=1 given x.

223
00:11:10.165 --> 00:11:14.965
We can also write probability
of y belongs to class 0.

224
00:11:14.965 --> 00:11:20.680
Given x is 1-probability(y)
= 1 given x.

225
00:11:20.680 --> 00:11:24.580
For example, the probability
of a customer staying with

226
00:11:24.580 --> 00:11:26.560
the company can be shown as

227
00:11:26.560 --> 00:11:29.020
probability of churn equals 1,

228
00:11:29.020 --> 00:11:31.270
given a customer's
income and age,

229
00:11:31.270 --> 00:11:34.675
which can be, for instance, 0.8.

230
00:11:34.675 --> 00:11:38.635
The probability of churn is
zero for the same customer,

231
00:11:38.635 --> 00:11:41.260
given a customer's
income and age can be

232
00:11:41.260 --> 00:11:45.140
calculated as 1-0.8 = 0.2.

233
00:11:47.510 --> 00:11:50.925
Now our job is to
train the model

234
00:11:50.925 --> 00:11:53.460
to set its parameter
values in such a way that

235
00:11:53.460 --> 00:11:59.105
our model is a good estimate
of probability(y)=1 given x.

236
00:11:59.105 --> 00:12:02.470
In fact, this is what a
good classifier model

237
00:12:02.470 --> 00:12:06.400
built by logistic regression
is supposed to do for us.

238
00:12:06.400 --> 00:12:08.875
Also, it should be
a good estimate

239
00:12:08.875 --> 00:12:11.800
of probability(y)
belongs to class 0

240
00:12:11.800 --> 00:12:17.545
given x that can be shown
as 1-sigmoid of Theta^T x.

241
00:12:17.545 --> 00:12:19.405
Now, the question is,

242
00:12:19.405 --> 00:12:21.445
how can we achieve this?

243
00:12:21.445 --> 00:12:24.685
We can find Theta through
the training process.

244
00:12:24.685 --> 00:12:27.610
Let's see what the
training process is.

245
00:12:27.610 --> 00:12:31.390
Step 1, initialize
Theta vector with

246
00:12:31.390 --> 00:12:34.870
random values as with most
machine learning algorithms.

247
00:12:34.870 --> 00:12:37.930
For example, -1 or 2.

248
00:12:37.930 --> 00:12:41.215
Step 2, calculate
the model output,

249
00:12:41.215 --> 00:12:43.550
which is Sigmoid of Theta^T

250
00:12:43.550 --> 00:12:46.525
x for a sample customer
in your training set.

251
00:12:46.525 --> 00:12:51.175
X and Theta^T x is the
feature vector values.

252
00:12:51.175 --> 00:12:54.085
For example, the age and
income of the customer,

253
00:12:54.085 --> 00:12:56.680
for instance, two and five.

254
00:12:56.680 --> 00:12:58.870
Theta is the confidence or

255
00:12:58.870 --> 00:13:01.405
weight that you've set
in the previous step.

256
00:13:01.405 --> 00:13:05.005
The output of this equation
is the prediction value.

257
00:13:05.005 --> 00:13:07.180
In other words, the probability

258
00:13:07.180 --> 00:13:09.790
that the customer
belongs to class 1.

259
00:13:09.790 --> 00:13:13.495
Step 3, compare the
output of our model,

260
00:13:13.495 --> 00:13:16.615
y hat, which could be
a value of, let's say,

261
00:13:16.615 --> 00:13:21.895
0.9997 with the actual
label of the customer,

262
00:13:21.895 --> 00:13:25.045
which is, for example,
one for churn.

263
00:13:25.045 --> 00:13:27.190
Then record the difference as

264
00:13:27.190 --> 00:13:29.440
our models error
for this customer,

265
00:13:29.440 --> 00:13:33.940
which would be 1-0.9997,

266
00:13:33.940 --> 00:13:38.290
which, of course, equals 0.0003.

267
00:13:38.290 --> 00:13:41.830
This is the error for
only one customer out of

268
00:13:41.830 --> 00:13:46.270
all the customers in the
training set. Step 4.

269
00:13:46.270 --> 00:13:49.600
Calculate the error for
all customers as we

270
00:13:49.600 --> 00:13:53.080
did in the previous steps
and add up these errors.

271
00:13:53.080 --> 00:13:55.360
The total error is the cost of

272
00:13:55.360 --> 00:13:59.335
your model and is calculated
by the models cost function.

273
00:13:59.335 --> 00:14:01.645
The cost function, by the way,

274
00:14:01.645 --> 00:14:03.280
basically represents how to

275
00:14:03.280 --> 00:14:05.275
calculate the error
of the model,

276
00:14:05.275 --> 00:14:06.790
which is the difference between

277
00:14:06.790 --> 00:14:10.960
the actual and the
models predicted values.

278
00:14:10.960 --> 00:14:13.720
The cost shows how
poorly the model

279
00:14:13.720 --> 00:14:16.000
is estimating the
customer's labels.

280
00:14:16.000 --> 00:14:18.520
Therefore, the lower the cost,

281
00:14:18.520 --> 00:14:20.230
the better the model is at

282
00:14:20.230 --> 00:14:23.125
estimating the customer's
labels correctly.

283
00:14:23.125 --> 00:14:26.110
What we want to do is
to try to minimize

284
00:14:26.110 --> 00:14:29.230
this cost. Step 5.

285
00:14:29.230 --> 00:14:31.060
But because the initial values

286
00:14:31.060 --> 00:14:33.145
for Theta were chosen randomly,

287
00:14:33.145 --> 00:14:36.850
it's very likely that the
cost function is very high.

288
00:14:36.850 --> 00:14:39.850
We change the theta in
such a way to hopefully

289
00:14:39.850 --> 00:14:43.465
reduce the total cost. Step 6.

290
00:14:43.465 --> 00:14:45.940
After changing the
values of Theta,

291
00:14:45.940 --> 00:14:47.920
we go back to step 2.

292
00:14:47.920 --> 00:14:50.530
Then we start another iteration

293
00:14:50.530 --> 00:14:53.185
and calculate the cost
of the model again,

294
00:14:53.185 --> 00:14:56.320
and we keep doing those
steps over and over,

295
00:14:56.320 --> 00:14:58.330
changing the values
of Theta each

296
00:14:58.330 --> 00:15:01.180
time until the cost
is low enough.

297
00:15:01.180 --> 00:15:04.210
This brings up two questions.

298
00:15:04.210 --> 00:15:07.060
First, how can we change
the values of Theta

299
00:15:07.060 --> 00:15:10.135
so that the cost is
reduced across iterations?

300
00:15:10.135 --> 00:15:13.855
Second, when should we
stop the iterations?

301
00:15:13.855 --> 00:15:17.200
There are different ways to
change the values of Theta,

302
00:15:17.200 --> 00:15:20.890
but one of the most popular
ways is gradient descent.

303
00:15:20.890 --> 00:15:24.955
Also, there are various
ways to stop iterations.

304
00:15:24.955 --> 00:15:28.270
But essentially, you stop
training by calculating

305
00:15:28.270 --> 00:15:30.100
the accuracy of your model and

306
00:15:30.100 --> 00:15:32.440
stop it when it's satisfactory.

307
00:15:32.440 --> 00:15:35.600
Thanks for watching this video.