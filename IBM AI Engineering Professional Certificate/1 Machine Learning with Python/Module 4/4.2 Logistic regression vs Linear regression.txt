Hello, and welcome.
In this video, we will learn the
difference between linear regression and
logistic regression. We go over linear regression
and see why it cannot be used properly for some
binary classification problems. We also look at the
sigmoid function, which is the main part
of logistic regression. Let's start. Let's look at the telecommunication
dataset again. The goal of logistic
regression is to build a model to
predict the class of each customer and
also the probability of each sample
belonging to a class. Ideally, we want
to build a model y hat that can estimate that the class
of a customer is one, given its features x. I want to emphasize that
y is the labels vector, also called actual values that
we would like to predict, and y hat is the vector of the predicted
values by our model. Mapping the class labels
to integer numbers, can we use linear regression
to solve this problem? First, let's recall how linear regression works to better understand
logistic regression. Forget about the churn prediction
for a minute and assume our goal is to predict the income of customers
in the dataset. This means that instead
of predicting churn, which is a categorical value, let's predict income, which is a continuous value.
How can we do this? Let's select an
independent variable, such as customer age and predict a dependent
variable, such as income. Of course, we can
have more features, but for the sake of simplicity, let's just take
one feature here. We can plot it and show age
as an independent variable, and income as the target value
we would like to predict. With linear regression,
you can fit a line or polynomial
through the data. We can find this line
through training our model or calculating
it mathematically, based on the sample sets. We'll say this is a straight
line through the sample set. This line has an equation
shown as a+bx_1. Now, use this line to predict
the continuous value y. That is, use this line
to predict the income of an unknown customer based on his or her age, and it is done. What if we want
to predict churn? Can we use the same
technique to predict a categorical field such
as churn? Let's see. Say we're given data
on customer churn, and our goal this
time is to predict the churn of customers
based on their age. We have a feature,
age denoted as x_1, and a categorical feature, churn with two classes, churn is yes, and churn is no. As mentioned, we can map yes and no to integer values
zero and one. How can we model it now? Well, graphically, we could represent our data
with a scatter plot, but this time, we have only
two values for the y-axis. In this plot, Class
0 is denoted in red, and Class 1 is denoted in blue. Our goal here is to
make a model based on existing data to predict if a new customer is red or blue. Let's do the same
technique that we used for linear
regression here to see if we can solve
the problem for a categorical attribute
such as churn. With linear regression,
you, again, can fit a polynomial
through the data, which is shown
traditionally as a+bx. This polynomial can also be shown traditionally as
Theta_0+Theta_1x_1. This line has two parameters which are shown
with vector Theta, where the values of the vector
are Theta_0 and Theta_1. We can also show the
equation of this line formally as Theta transpose X. Generally, we can show the equation for a
multidimensional space, as Theta transpose X, where Theta is the
parameters of the line in two-dimensional space
or parameters of a plane in three-dimensional
space, and so on. As Theta is a vector of parameters and is supposed
to be multiplied by x, it is shown conventionally
as transpose Theta. Theta is also called
the weights factor or confidences of the equation with both these terms
used interchangeably, and x is the feature set, which represents a customer. Anyway, given a dataset, all the feature sets, x, Theta parameters can
be calculated through an optimization algorithm
or mathematically, which results in the equation
of the fitting line. For example, the parameters
of this line are -1 and 0.1, and the equation for
the line is -1+0.1x_1. Now, we can use this regression line to predict the churn of the new customer. For example, for our customer, or let's say a data point with
x value of age equals 13, we can plug the value
into the line formula and the y value is calculated
and returns a number. For instance, for p_1 point, we have Theta transpose
x equals -1+0.1*x_1 = -1+0.1*13 = 0.3. We can show it on our graph. Now we can define
a threshold here. For example, at 0.5
to define the class. We write a rule here
for our model, y hat, which allows us to separate
class 0 from class 1. If the value of Theta
transpose x is less than 0.5, then the class is zero. Otherwise, if the value of Theta transpose x
is more than 0.5, then the class is one. Because our customer's y value is less than the threshold, we can say it belongs to
class 0 based on our model. But there is one problem here. What is the probability that this customer
belongs to class 0? As you can see, it's not the best model to
solve this problem. Also, there are some
other issues which verify that linear regression is not the proper method for
classification problems. As mentioned, if we use the regression line to
calculate the class of a point, it always returns a number
such as 3 or -2, and so on. Then we should use a threshold. For example, 0.5 to assign that point to either
class of zero or one. This threshold works as
a step function that outputs zero or one regardless
of how big or small, positive or negative
the input is. Using the threshold, we can
find the class of a record. Notice that in the
step function, no matter how big the value is as long as it's
greater than 0.5, it simply equals
one and vice versa, regardless of how
small the value y is, the output would be zero
if it is less than 0.5. In other words, there is
no difference between a customer who has a
value of one or 1,000. The outcome would be one. Instead of having
this step function, wouldn't it be nice if
we had a smother line. One that would project these
values between 0 and 1. Indeed, the existing method
does not really give us the probability of a
customer belonging to a class, which is very desirable. We need a method
that can give us the probability of falling
in a class as well. What is the scientific
solution here? Well, if instead of
using Theta transpose x, we use a specific
function called sigmoid. Then sigmoid of Theta transpose x gives us the probability of a point belonging to a class instead of the value
of y directly. I'll explain this sigmoid
function in a second. But for now, please accept
that it will do the trick. Instead of calculating
the value of Theta transpose x directly, it returns the
probability that a Theta transpose x is
very big or very small. It always returns a
value between 0 and 1 depending on how large the
Theta transpose x actually is. Now, our model is sigmoid
of Theta transpose x, which represents the
probability that the output is one given x. Now, the question is, what is the sigmoid function? Let me explain in detail
what sigmoid really is. The sigmoid function,
also called the logistic function resembles the step function and is used by the following expression in
the logistic regression. The sigmoid function looks
a bit complicated at first, but don't worry about
remembering this equation. It'll make sense to you
after working with it. Notice that in the
sigmoid equation, when Theta transpose
x gets very big, the e power minus theta transpose x in the
denominator of the fraction becomes almost zero and the value of the sigmoid
function gets closer to one. If Theta transpose
x is very small, the sigmoid function
gets closer to zero. Depicting on the sigmoid plot, when Theta transpose
x gets bigger, the value of the sigmoid
function gets closer to one. Also, if the Theta
transpose x is very small, the sigmoid function
gets closer to zero. The sigmoid functions output
is always between 0 and 1, which makes it proper to interpret the results
as probabilities. It is obvious that
when the outcome of the sigmoid function
gets closer to 1, the probability(y)=1
given x goes up. In contrast, when the sigmoid
value is closer to zero, the probability(y)=1,
given x, is very small. What is the output of our model when we use
the sigmoid function? In logistic regression, we
model the probability that an input x belongs to
the default class, y=1, and we can
write this formally as probability(y)=1 given x. We can also write probability
of y belongs to class 0. Given x is 1-probability(y)
= 1 given x. For example, the probability
of a customer staying with the company can be shown as probability of churn equals 1, given a customer's
income and age, which can be, for instance, 0.8. The probability of churn is
zero for the same customer, given a customer's
income and age can be calculated as 1-0.8 = 0.2. Now our job is to
train the model to set its parameter
values in such a way that our model is a good estimate
of probability(y)=1 given x. In fact, this is what a
good classifier model built by logistic regression
is supposed to do for us. Also, it should be
a good estimate of probability(y)
belongs to class 0 given x that can be shown
as 1-sigmoid of Theta^T x. Now, the question is, how can we achieve this? We can find Theta through
the training process. Let's see what the
training process is. Step 1, initialize
Theta vector with random values as with most
machine learning algorithms. For example, -1 or 2. Step 2, calculate
the model output, which is Sigmoid of Theta^T x for a sample customer
in your training set. X and Theta^T x is the
feature vector values. For example, the age and
income of the customer, for instance, two and five. Theta is the confidence or weight that you've set
in the previous step. The output of this equation
is the prediction value. In other words, the probability that the customer
belongs to class 1. Step 3, compare the
output of our model, y hat, which could be
a value of, let's say, 0.9997 with the actual
label of the customer, which is, for example,
one for churn. Then record the difference as our models error
for this customer, which would be 1-0.9997, which, of course, equals 0.0003. This is the error for
only one customer out of all the customers in the
training set. Step 4. Calculate the error for
all customers as we did in the previous steps
and add up these errors. The total error is the cost of your model and is calculated
by the models cost function. The cost function, by the way, basically represents how to calculate the error
of the model, which is the difference between the actual and the
models predicted values. The cost shows how
poorly the model is estimating the
customer's labels. Therefore, the lower the cost, the better the model is at estimating the customer's
labels correctly. What we want to do is
to try to minimize this cost. Step 5. But because the initial values for Theta were chosen randomly, it's very likely that the
cost function is very high. We change the theta in
such a way to hopefully reduce the total cost. Step 6. After changing the
values of Theta, we go back to step 2. Then we start another iteration and calculate the cost
of the model again, and we keep doing those
steps over and over, changing the values
of Theta each time until the cost
is low enough. This brings up two questions. First, how can we change
the values of Theta so that the cost is
reduced across iterations? Second, when should we
stop the iterations? There are different ways to
change the values of Theta, but one of the most popular
ways is gradient descent. Also, there are various
ways to stop iterations. But essentially, you stop
training by calculating the accuracy of your model and stop it when it's satisfactory. Thanks for watching this video.