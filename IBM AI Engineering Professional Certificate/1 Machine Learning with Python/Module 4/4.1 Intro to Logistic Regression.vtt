WEBVTT

1
00:00:01.310 --> 00:00:03.270
Hello and welcome.

2
00:00:03.270 --> 00:00:06.319
In this video,
we'll learn a machine learning method called

3
00:00:06.319 --> 00:00:10.380
Logistic Regression which is used for classification.

4
00:00:10.380 --> 00:00:15.670
In examining this method, we'll specifically
answer these three questions.

5
00:00:15.670 --> 00:00:17.990
What is logistic regression?

6
00:00:17.990 --> 00:00:21.890
What kind of problems can be solved by logistic
regression?

7
00:00:21.890 --> 00:00:25.570
In which situations do we use logistic regression?

8
00:00:25.570 --> 00:00:28.499
So let's get started.

9
00:00:28.499 --> 00:00:33.320
Logistic regression is a statistical and machine
learning technique for classifying records

10
00:00:33.320 --> 00:00:37.520
of a dataset based on the values of the input
fields.

11
00:00:37.520 --> 00:00:41.360
Let's say we have a telecommunication dataset
that we'd like to

12
00:00:41.360 --> 00:00:46.080
analyze in order to understand which customers
might leave us next month.

13
00:00:46.080 --> 00:00:52.710
This is historical customer data where each
row represents one customer.

14
00:00:52.710 --> 00:00:57.660
Imagine that you're an analyst at this company
and you have to find out who is leaving and

15
00:00:57.660 --> 00:00:59.190
why?

16
00:00:59.190 --> 00:01:01.880
You'll use the dataset to build a model based
on

17
00:01:01.880 --> 00:01:07.749
historical records and use it to predict the
future churn within the customer group.

18
00:01:07.749 --> 00:01:10.810
The dataset includes information about services
that

19
00:01:10.810 --> 00:01:16.630
each customer has signed up for, customer
account information, demographic information

20
00:01:16.630 --> 00:01:22.170
about customers like gender and age range
and also customers who've left the company

21
00:01:22.170 --> 00:01:24.049
within the last month.

22
00:01:24.049 --> 00:01:26.600
The column is called churn.

23
00:01:26.600 --> 00:01:29.340
We can use logistic regression to build a
model for

24
00:01:29.340 --> 00:01:33.790
predicting customer churn using the given
features.

25
00:01:33.790 --> 00:01:39.369
In logistic regression, we use one or more
independent variables such as tenure, age,

26
00:01:39.369 --> 00:01:45.329
and income to predict an outcome, such as
churn, which we call the dependent variable

27
00:01:45.329 --> 00:01:50.360
representing whether or not customers will
stop using the service.

28
00:01:50.360 --> 00:01:55.380
Logistic regression is analogous to linear
regression but tries to predict a categorical

29
00:01:55.380 --> 00:01:59.450
or discrete target field instead of a numeric
one.

30
00:01:59.450 --> 00:02:04.579
In linear regression, we might try to predict
a continuous value of variables such as the

31
00:02:04.579 --> 00:02:10.319
price of a house, blood pressure of a patient,
or fuel consumption of a car.

32
00:02:10.319 --> 00:02:18.500
But in logistic regression, we predict a variable
which is binary such as yes/no, true/false,

33
00:02:18.500 --> 00:02:21.530
successful or not successful, pregnant/not
pregnant,

34
00:02:21.530 --> 00:02:26.870
and so on, all of which can be coded as zero
or one.

35
00:02:26.870 --> 00:02:31.760
In logistic regression independent variables
should be continuous.

36
00:02:31.760 --> 00:02:35.930
If categorical, they should be dummy or indicator
coded.

37
00:02:35.930 --> 00:02:40.909
This means we have to transform them to some
continuous value.

38
00:02:40.909 --> 00:02:47.019
Please note that logistic regression can be
used for both binary classification and multi-class

39
00:02:47.019 --> 00:02:48.200
classification.

40
00:02:48.200 --> 00:02:54.599
But for simplicity in this video,
we'll focus on binary classification.

41
00:02:54.599 --> 00:03:00.520
Let's examine some applications of logistic
regression before we explain how they work.

42
00:03:00.520 --> 00:03:05.629
As mentioned, logistic regression is a type
of classification algorithm, so it can be

43
00:03:05.629 --> 00:03:07.980
used in different situations.

44
00:03:07.980 --> 00:03:12.760
For example, to predict the probability of
a person having a heart attack within a specified

45
00:03:12.760 --> 00:03:16.480
time period, based on our knowledge of the
person's age,

46
00:03:16.480 --> 00:03:19.510
sex, and body mass index.

47
00:03:19.510 --> 00:03:24.840
Or to predict the chance of mortality in an
injured patient or to predict whether a patient

48
00:03:24.840 --> 00:03:28.480
has a given disease such as diabetes based
on

49
00:03:28.480 --> 00:03:32.519
observed characteristics of that patient such
as weight,

50
00:03:32.519 --> 00:03:37.239
height, blood pressure, and results of various
blood tests and so on.

51
00:03:37.239 --> 00:03:42.630
In a marketing context, we can use it to predict
the likelihood of a customer purchasing

52
00:03:42.630 --> 00:03:48.180
a product or halting a subscription as we've
done in our churn example.

53
00:03:48.180 --> 00:03:53.200
We can also use logistic regression to predict
the probability of failure of a given process,

54
00:03:53.200 --> 00:03:55.120
system or product.

55
00:03:55.120 --> 00:04:00.269
We can even use it to predict the likelihood
of a homeowner defaulting on a mortgage.

56
00:04:00.269 --> 00:04:05.489
These are all good examples of problems that
can be solved using logistic regression.

57
00:04:05.489 --> 00:04:10.530
Notice that in all these examples not only
do we predict the class of each case,

58
00:04:10.530 --> 00:04:16.310
we also measure the probability of a case
belonging to a specific class.

59
00:04:16.310 --> 00:04:22.010
There are different machine algorithms which
can classify or estimate a variable.

60
00:04:22.010 --> 00:04:25.310
The question is, when should we use logistic
regression?

61
00:04:25.310 --> 00:04:30.759
Here are four situations in which logistic
regression is a good candidate.

62
00:04:30.759 --> 00:04:37.830
First, when the target field in your data
is categorical or specifically is binary.

63
00:04:37.830 --> 00:04:44.860
Such as zero/one, yes/no, churn or no churn,
positive/negative and so on.

64
00:04:44.860 --> 00:04:48.440
Second, you need the probability of your prediction.

65
00:04:48.440 --> 00:04:54.760
For example, if you want to know what the
probability is of a customer buying a product.

66
00:04:54.760 --> 00:04:59.860
Logistic regression returns a probability
score between zero and one for a given sample

67
00:04:59.860 --> 00:05:00.860
of data.

68
00:05:00.860 --> 00:05:06.830
In fact, logistic regression predicts the
probability of that sample and we map the

69
00:05:06.830 --> 00:05:10.640
cases to a discrete class based on that probability.

70
00:05:10.640 --> 00:05:14.669
Third, if your data is linearly separable.

71
00:05:14.669 --> 00:05:20.910
The decision boundary of logistic regression
is a line or a plane or a hyper plane.

72
00:05:20.910 --> 00:05:25.960
A classifier will classify all the points
on one side of the decision boundary as belonging

73
00:05:25.960 --> 00:05:31.880
to one class and all those on the other side
as belonging to the other class.

74
00:05:31.880 --> 00:05:38.660
For example, if we have just two features
and are not applying any polynomial processing

75
00:05:38.660 --> 00:05:46.849
we can obtain an inequality like Theta zero
plus Theta 1x1 plus theta 2x2 is greater than

76
00:05:46.849 --> 00:05:51.030
zero,
which is a half-plane easily plottable.

77
00:05:51.030 --> 00:05:57.160
Please note that in using logistic regression,
we can also achieve a complex decision boundary

78
00:05:57.160 --> 00:06:02.490
using polynomial processing as well,
which is out of scope here.

79
00:06:02.490 --> 00:06:06.230
You'll get more insight from
decision boundaries when you understand how

80
00:06:06.230 --> 00:06:08.620
logistic regression works.

81
00:06:08.620 --> 00:06:12.940
Fourth, you need to understand the impact
of a feature.

82
00:06:12.940 --> 00:06:17.040
You can select the best features based on
the statistical significance of the logistic

83
00:06:17.040 --> 00:06:19.780
regression model coefficients or parameters.

84
00:06:19.780 --> 00:06:26.680
That is, after finding the optimum parameters,
a feature X with the weight Theta one close

85
00:06:26.680 --> 00:06:30.620
to zero has
a smaller effect on the prediction than features

86
00:06:30.620 --> 00:06:34.020
with large absolute values of Theta one.

87
00:06:34.020 --> 00:06:38.530
Indeed, it allows us to understand the impact
an independent variable

88
00:06:38.530 --> 00:06:44.760
has on the dependent variable while controlling
other independent variables.

89
00:06:44.760 --> 00:06:47.070
Let's look at our dataset again.

90
00:06:47.070 --> 00:06:51.300
We defined the independent variables as X
and dependent variable as Y.

91
00:06:51.300 --> 00:06:57.650
Notice, that for the sake of simplicity we
can code the target or dependent values to

92
00:06:57.650 --> 00:07:00.060
zero or one.

93
00:07:00.060 --> 00:07:05.289
The goal of logistic regression is to build
a model to predict the class of each sample

94
00:07:05.289 --> 00:07:09.850
which in this case is a customer, as well
as the probability of each sample belonging

95
00:07:09.850 --> 00:07:12.009
to a class.

96
00:07:12.009 --> 00:07:15.550
Given that, let's start to formalize the problem.

97
00:07:15.550 --> 00:07:20.659
X is our dataset in the space of real numbers
of m by n.

98
00:07:20.659 --> 00:07:27.330
That is, of m dimensions or features and n
records, and Y is the class that we want to

99
00:07:27.330 --> 00:07:31.150
predict,
which can be either zero or one.

100
00:07:31.150 --> 00:07:38.080
Ideally, a logistic regression model, so-called
Y hat, can predict that the class of the customer

101
00:07:38.080 --> 00:07:40.750
is one, given its features X.

102
00:07:40.750 --> 00:07:46.389
It can also be shown quite easily that the
probability of a customer being in class zero

103
00:07:46.389 --> 00:07:53.270
can be calculated as one minus the probability
that the class of the customer is one.

104
00:07:53.270 --> 00:07:54.739
Thanks for watching this video.

105
00:07:54.739 --> 00:08:00.000
(Music)