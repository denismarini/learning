Salve e benvenuti. In questo video, tratteremo K-Means Clustering. Quindi iniziamo. Immagina di avere un set di dati sui clienti e di dover applicare la segmentazione dei clienti su questi dati storici. La segmentazione dei clienti è la pratica di suddividere una base clienti in gruppi di individui con caratteristiche simili. Uno degli algoritmi che possono essere utilizzati per la segmentazione dei clienti è il clustering K-Means. K-Means può raggruppare i dati solo senza supervisione in base alla somiglianza dei clienti tra loro. Definiamo questa tecnica in modo più formale. Esistono vari tipi di algoritmi di clustering come il partizionamento, il clustering gerarchico o basato sulla densità. K-Means è un tipo di clustering di partizionamento, ovvero divide i dati in K sottoinsiemi o cluster non sovrapposti senza alcuna struttura o etichetta interna del cluster. Ciò significa che si tratta di un algoritmo non supervisionato. Gli oggetti all'interno di un cluster sono molto simili e gli oggetti di cluster diversi sono molto diversi o dissimili. Come puoi vedere, per usare K-Means dobbiamo trovare esempi simili: ad esempio clienti simili. Ora affrontiamo un paio di domande chiave. Innanzitutto, come possiamo trovare la somiglianza dei campioni nel clustering e poi come possiamo misurare la somiglianza tra due clienti per quanto riguarda i loro dati demografici? Sebbene l'obiettivo di K-Means sia quello di formare cluster in modo tale che campioni simili entrino in un cluster e campioni diversi rientrino in cluster diversi, si può dimostrare che invece di una metrica di somiglianza, possiamo utilizzare metriche di dissomiglianza. In altre parole, convenzionalmente viene utilizzata la distanza dei campioni l'uno dall'altro per modellare i cluster. Quindi possiamo dire che K-Means cerca di ridurre al minimo le distanze all'interno del cluster e massimizzare le distanze tra i cluster. Ora, la domanda è: come possiamo calcolare la differenza o la distanza di due casi, ad esempio due clienti? Supponiamo di avere due clienti, li chiameremo Clienti uno e due. Supponiamo inoltre di avere una sola funzionalità per ciascuno di questi due clienti e che tale caratteristica sia l'età. Possiamo facilmente utilizzare un tipo specifico di distanza Minkowski per calcolare la distanza di questi due clienti. In effetti, è la distanza euclidea. E se avessimo più di una funzionalità, ad esempio età e reddito? Ad esempio, se abbiamo reddito ed età per ogni cliente, possiamo comunque utilizzare la stessa formula ma questa volta in uno spazio bidimensionale. Inoltre, possiamo usare la stessa matrice di distanza per i vettori multidimensionali. Naturalmente, dobbiamo normalizzare il nostro set di funzionalità per ottenere una misura di dissomiglianza accurata. Esistono anche altre misure di dissomiglianza che possono essere utilizzate a questo scopo, ma dipende in larga misura dal tipo di dati e anche dal dominio in cui viene eseguito il clustering. Ad esempio, puoi usare la distanza euclidea, la somiglianza del coseno , la distanza media e così via. In effetti, la misura di somiglianza controlla fortemente il modo in cui si formano i cluster, quindi si consiglia di conoscere a fondo il set di dati e il tipo di dati delle feature e quindi scegliere la misurazione della distanza significativa. Vediamo ora come funziona il clustering K-Means. Per semplicità, supponiamo che il nostro set di dati abbia solo due caratteristiche: l'età e il reddito dei clienti. Ciò significa che è uno spazio bidimensionale. Possiamo mostrare la distribuzione dei clienti utilizzando un grafico a dispersione: l'asse Y indica l'età e l' asse X mostra il reddito dei clienti. Cerchiamo di raggruppare il set di dati dei clienti in gruppi o cluster distinti in base a queste due dimensioni. Nella prima fase, dovremmo determinare il numero di cluster. Il concetto chiave dell'algoritmo K-Means è che sceglie casualmente un punto centrale per ogni cluster. Significa che dobbiamo inizializzare K che rappresenta il numero di cluster. Essenzialmente, determinare il numero di cluster in un set di dati o K è un problema difficile in K-Means, di cui parleremo più avanti. Per ora, mettiamo K uguale a tre per il nostro set di dati di esempio. È come se avessimo tre punti rappresentativi per i nostri cluster. Questi tre punti dati sono chiamati centroidi dei cluster e dovrebbero avere le stesse dimensioni del set di funzionalità del nostro set di funzionalità per i clienti. Esistono due approcci per scegliere questi centroidi. Primo, possiamo scegliere casualmente tre osservazioni dal set di dati e utilizzare queste osservazioni come media iniziale. O due, possiamo creare tre punti casuali come centroidi dei cluster, una nostra scelta mostrata nel grafico con il colore rosso. Dopo la fase di inizializzazione che consisteva nella definizione del baricentro di ogni cluster, dobbiamo assegnare ogni cliente al centro più vicino. A tal fine, dobbiamo calcolare la distanza di ogni punto dati o, nel nostro caso, di ogni cliente dai punti centroidi. Come accennato in precedenza, a seconda della natura dei dati e dello scopo per cui viene utilizzato il clustering, è possibile utilizzare diverse misure di distanza per posizionare gli elementi nei cluster. Pertanto, si formerà una matrice in cui ogni riga rappresenta la distanza di un cliente da ciascun centroide. Si chiama Distance Matrix. L' obiettivo principale del clustering K-Means è ridurre al minimo la distanza dei punti dati dal baricentro di questo cluster e massimizzare la distanza dagli altri centroidi del cluster. Quindi, in questa fase, dobbiamo trovare il baricentro più vicino a ciascun punto dati. Possiamo usare la matrice delle distanze per trovare il baricentro più vicino ai punti dati. Trovando i centroidi più vicini per ogni punto dati, assegniamo ogni punto dati a quel cluster. In altre parole, tutti i clienti rientreranno in un cluster in base alla loro distanza dai centroidi. Possiamo facilmente affermare che non si ottengono buoni cluster perché i centroidi sono stati scelti casualmente sin dall'inizio. In effetti, il modello avrebbe un errore elevato. Qui, l'errore è la distanza totale di ogni punto dal suo baricentro. Può essere visualizzato come errore relativo alla somma dei quadrati all'interno del cluster. Intuitivamente, cerchiamo di ridurre questo errore. Significa che dovremmo modellare i cluster in modo tale da ridurre al minimo la distanza totale di tutti i membri di un cluster dal suo baricentro. Ora, la domanda è: come possiamo trasformarlo in cluster migliori con meno errori? Ok, spostiamo i centroidi. Nella fase successiva, ogni centro cluster verrà aggiornato per fungere da base per i punti dati del relativo cluster. In effetti, ogni centroide si muove in base ai membri del cluster. In altre parole, il baricentro di ciascuno dei tre cluster diventa la nuova media. Ad esempio, se la coordinazione del punto A è 7,4 e 3,6 e le caratteristiche dei punti B sono 7,8 e 3,8, il nuovo baricentro di questo cluster con due punti sarebbe la media di essi, ovvero 7,6 e 3,7. Ora abbiamo nuovi centroidi. Come puoi immaginare, ancora una volta dovremo calcolare la distanza di tutti i punti dai nuovi centroidi. I punti vengono raggruppati e i centroidi si spostano nuovamente. Ciò continua fino a quando i baricentri non si muovono più. Tieni presente che ogni volta che un baricentro si sposta, la distanza di ogni punto dal baricentro deve essere misurata nuovamente. Sì, K-Means è un algoritmo iterativo e dobbiamo ripetere i passaggi da due a quattro finché l'algoritmo non converge. In ogni iterazione, sposterà i centroidi, calcolerà le distanze dai nuovi centroidi e assegnerà punti dati al centroide più vicino. Ne risulteranno i cluster con un errore minimo o i cluster più densi. Tuttavia, trattandosi di un algoritmo euristico, non vi è alcuna garanzia che converga verso l'ottimale globale e il risultato può dipendere dai cluster iniziali. Significa che è garantita la convergenza di questo algoritmo verso un risultato, ma il risultato potrebbe essere un ottimale locale, cioè non necessariamente il miglior risultato possibile. Per risolvere questo problema, è comune eseguire l'intero processo più volte con condizioni iniziali diverse. Ciò significa che con i centroidi iniziali randomizzati, può dare un risultato migliore. Poiché l'algoritmo è generalmente molto veloce, non sarebbe un problema eseguirlo più volte. Grazie per aver guardato questo video.